"""
Tool: bo_suggest
Generated by Tool Builder Agent

Bayesian optimization tool that fits/updates a surrogate model (e.g., Gaussian Process) on experimental data and suggests next experiments to run. Supports flexible parameter spaces, multiple acquisition functions, and both continuous and categorical parameters. Handles datasets, parameter spaces, and constraints in various formats for maximum flexibility.

Examples:
1. bo_suggest('experiments', 'next_experiments', param_space={'temperature': [20, 100], 'catalyst': ['A', 'B', 'C'], 'pressure': [1, 5]}, n=3, acquisition='EI', response='yield')
2. bo_suggest('reaction_data', 'bo_suggestions', candidate_pool='candidate_experiments', n=5, acquisition='UCB', maximize=False, random_seed=42)

Data Inputs: dataset (string): Name of dataset containing experimental results with parameter settings and outcomes. Optional: candidate_pool (string): Name of dataset with pre-defined candidate experiments to choose from. features (string): Name of dataset or column specification for feature engineering.
Data Outputs: out_dataset (string): Name for output dataset containing suggested experiments with parameter settings, predicted outcomes, uncertainties, and acquisition scores.
"""

import numpy as np
import pandas as pd
import pyarrow as pa
from typing import Any

# Tool implementation
def bo_suggest(train_dataset, predict_dataset, out_dataset, response, n=5, acquisition='EI', maximize=True, random_seed=None, *, db, config) -> str:
    """
    Bayesian optimization suggestion tool (train on one dataset, select from another).

    - Reads training data (with response column) and prediction/candidate data from db
    - Treats all non-response columns as features; categorical columns are one-hot encoded
    - Fits a Gaussian Process surrogate on training features/response
    - Predicts on prediction data and selects top-n candidates by acquisition
    - Saves selected candidates with diagnostics to the db

    Args:
        train_dataset: Name of dataset with experimental results (must include `response`)
        predict_dataset: Name of dataset with candidate rows to score/select
        out_dataset: Name for output dataset with selected suggestions
        response: Name of the response/objective column in training data
        n: Number of suggestions to generate (default: 5)
        acquisition: Acquisition function ('EI', 'UCB', 'PI', default: 'EI')
        maximize: Whether to maximize objective (default: True)
        random_seed: Random seed for reproducibility (optional)
    """
    import numpy as np
    import pandas as pd
    from sklearn.gaussian_process import GaussianProcessRegressor
    from sklearn.gaussian_process.kernels import Matern, WhiteKernel
    from sklearn.preprocessing import StandardScaler
    from scipy.stats import norm

    try:
        # Load data
        try:
            df_train = db.get_table(train_dataset).to_pandas()
        except Exception as e:
            return f"Error loading training dataset '{train_dataset}': {str(e)}"
        try:
            df_pred = db.get_table(predict_dataset).to_pandas()
        except Exception as e:
            return f"Error loading prediction dataset '{predict_dataset}': {str(e)}"

        if df_train.empty:
            return f"Training dataset '{train_dataset}' is empty"
        if df_pred.empty:
            return f"Prediction dataset '{predict_dataset}' is empty"
        if response not in df_train.columns:
            return f"Response column '{response}' not in training data. Columns: {list(df_train.columns)}"

        # Set seed
        if random_seed is not None:
            np.random.seed(random_seed)

        # Prepare y
        y = df_train[response]
        try:
            y = pd.to_numeric(y)
        except Exception:
            return f"Response column '{response}' must be numeric or coercible to numeric"
        if not maximize:
            y = -y

        # Feature columns = all except response. For predict data, response may be absent.
        feature_cols_train = [c for c in df_train.columns if c != response]
        feature_cols_pred = [c for c in df_pred.columns if c != response]
        feature_cols = sorted(set(feature_cols_train).union(feature_cols_pred))
        if not feature_cols:
            return "No feature columns found besides the response column"

        X_train_raw = df_train.reindex(columns=feature_cols, fill_value=np.nan)
        X_pred_raw = df_pred.reindex(columns=feature_cols, fill_value=np.nan)

        # Determine numeric vs categorical based on train+pred combined coercion
        numeric_cols = []
        categorical_cols = []
        for col in feature_cols:
            s_train = X_train_raw[col]
            s_pred = X_pred_raw[col]
            s_all = pd.concat([s_train, s_pred], ignore_index=True)
            if pd.api.types.is_numeric_dtype(s_all):
                numeric_cols.append(col)
            else:
                coerced = pd.to_numeric(s_all, errors='coerce')
                if coerced.notnull().mean() >= 0.95:
                    # Use numeric coercion for both train and pred
                    X_train_raw[col] = pd.to_numeric(X_train_raw[col], errors='coerce')
                    X_pred_raw[col] = pd.to_numeric(X_pred_raw[col], errors='coerce')
                    numeric_cols.append(col)
                else:
                    categorical_cols.append(col)

        # Numeric imputation using train medians
        X_train_num = X_train_raw[numeric_cols].copy() if numeric_cols else pd.DataFrame(index=X_train_raw.index)
        X_pred_num = X_pred_raw[numeric_cols].copy() if numeric_cols else pd.DataFrame(index=X_pred_raw.index)
        if not X_train_num.empty:
            medians = X_train_num.median(axis=0, skipna=True)
            X_train_num = X_train_num.fillna(medians)
            X_pred_num = X_pred_num.fillna(medians)

        # One-hot encode categoricals on combined to align columns
        if categorical_cols:
            train_cat = X_train_raw[categorical_cols].astype(str).fillna("__MISSING__")
            pred_cat = X_pred_raw[categorical_cols].astype(str).fillna("__MISSING__")
            cat_all = pd.concat([train_cat, pred_cat], axis=0, ignore_index=True)
            dummies_all = pd.get_dummies(cat_all, prefix=categorical_cols, dtype=float)
            # Split back
            n_train = len(train_cat)
            X_train_dum = dummies_all.iloc[:n_train, :].reset_index(drop=True)
            X_pred_dum = dummies_all.iloc[n_train:, :].reset_index(drop=True)
        else:
            X_train_dum = pd.DataFrame(index=X_train_raw.index)
            X_pred_dum = pd.DataFrame(index=X_pred_raw.index)

        # Build final matrices
        X_train = pd.concat([X_train_num.reset_index(drop=True), X_train_dum], axis=1)
        X_pred = pd.concat([X_pred_num.reset_index(drop=True), X_pred_dum], axis=1)

        # Align columns in case of any mismatch (should not happen due to combined dummies)
        X_pred = X_pred.reindex(columns=X_train.columns, fill_value=0.0)

        # Drop rows with NaN in train features or y
        valid_mask = X_train.notnull().all(axis=1) & y.notnull()
        X_train = X_train.loc[valid_mask]
        y = y.loc[valid_mask]
        if len(X_train) < 2:
            return "Not enough valid training rows after cleaning"

        # Scale numeric columns only
        from sklearn.preprocessing import StandardScaler
        scaler = None
        if numeric_cols:
            scaler = StandardScaler()
            # Scale via pandas to avoid dtype casting issues
            X_train_scaled = X_train.copy()
            X_pred_scaled = X_pred.copy()
            X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
            X_pred_scaled[numeric_cols] = scaler.transform(X_pred[numeric_cols])
        else:
            X_train_scaled = X_train
            X_pred_scaled = X_pred

        # Fit GP and predict
        kernel = Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=1e-5)
        gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, n_restarts_optimizer=5)
        gp.fit(X_train_scaled.values.astype(float), y.values.astype(float))

        mu, sigma = gp.predict(X_pred_scaled.values.astype(float), return_std=True)
        sigma = np.maximum(sigma, 1e-9)
        if acquisition.upper() == 'EI':
            best = np.max(y.values)
            z = (mu - best) / sigma
            acq = (mu - best) * norm.cdf(z) + sigma * norm.pdf(z)
        elif acquisition.upper() == 'UCB':
            beta = 2.0
            acq = mu + beta * sigma
        elif acquisition.upper() == 'PI':
            best = np.max(y.values)
            z = (mu - best) / sigma
            acq = norm.cdf(z)
        else:
            return f"Unsupported acquisition function: {acquisition}. Use 'EI', 'UCB', or 'PI'."

        # Select top n indices from prediction dataset
        top_idx = np.argsort(acq)[-n:][::-1]
        suggestions = df_pred.iloc[top_idx].copy().reset_index(drop=True)
        suggestions['predicted_mean'] = mu[top_idx]
        suggestions['predicted_std'] = sigma[top_idx]
        suggestions['acquisition_score'] = acq[top_idx]

        # If minimizing, flip sign back for interpretability
        if not maximize:
            suggestions['predicted_mean'] = -suggestions['predicted_mean']

        db.save_table(out_dataset, suggestions)
        return (
            f"Selected {len(suggestions)} suggestions from '{predict_dataset}' using {acquisition}. "
            f"Surrogate trained on {len(X_train)} rows from '{train_dataset}'. Response: '{response}'. "
            f"Saved to '{out_dataset}'."
        )

    except Exception as e:
        return f"Error in bo_suggest: {str(e)}"

# Tool metadata for registration
TOOL_METADATA = {'name': 'bo_suggest', 'description': 'Bayesian optimization tool that fits/updates a surrogate model (e.g., Gaussian Process) on experimental data and suggests next experiments to run. Supports flexible parameter spaces, multiple acquisition functions, and both continuous and categorical parameters. Handles datasets, parameter spaces, and constraints in various formats for maximum flexibility.', 'examples': ["bo_suggest('experiments', 'next_experiments', param_space={'temperature': [20, 100], 'catalyst': ['A', 'B', 'C'], 'pressure': [1, 5]}, n=3, acquisition='EI', response='yield')", "bo_suggest('reaction_data', 'bo_suggestions', candidate_pool='candidate_experiments', n=5, acquisition='UCB', maximize=False, random_seed=42)"], 'data_inputs': 'dataset (string): Name of dataset containing experimental results with parameter settings and outcomes. Optional: candidate_pool (string): Name of dataset with pre-defined candidate experiments to choose from. features (string): Name of dataset or column specification for feature engineering.', 'data_outputs': 'out_dataset (string): Name for output dataset containing suggested experiments with parameter settings, predicted outcomes, uncertainties, and acquisition scores.'}
